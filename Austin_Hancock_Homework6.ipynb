{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Austin Hancock\n",
    "\n",
    "## MSDS 7337 - Section 401\n",
    "## Homework - #6\n",
    "[Data Science @ Southern Methodist University](https://datascience.smu.edu/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "* [Description](#Description)\n",
    "* [Tools](#Tools)\n",
    "* [Question-1](#Question-1)\n",
    "* [Question-2](#Question-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"Description\"></a>Description\n",
    "For the HW-6 assignment I will be addressing the following:\n",
    "\n",
    "    - Evaluate the text similarity of book titles from Amazon\n",
    "    - Evaluate the text similarity of book titles from a major search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"Tools\"></a>Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows-10-10.0.17134-SP0\n",
      "Python 3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)]\n",
      "NLTK 3.2.4\n",
      "bs4 4.6.0\n",
      "re 2.2.1\n",
      "sklearn 0.19.1\n"
     ]
    }
   ],
   "source": [
    "import platform; print(platform.platform())\n",
    "import sys; print(\"Python\", sys.version)\n",
    "import nltk; print(\"NLTK\", nltk.__version__)\n",
    "import bs4; print(\"bs4\", bs4.__version__)\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib import request\n",
    "import re; print(\"re\", re.__version__)\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sklearn; print(\"sklearn\", sklearn.__version__)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"Question-1\"></a>Question-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the text similarity of Amazon book search results by doing the following: \n",
    "\n",
    "    - Do a book search on Amazon and manually copy the top 24 titles\n",
    "    - Run a text-similarity measure and compare each of the book titles, pairwise, to every other one\n",
    "    - Find the two titles which are most similar/dissimilar and where they rank amongst the top results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project I will be compiling a list of books using the search criteria of 'asteroids'.\n",
    "The first step is to pass this search criteria into the Amazon search control and specifiy that the search should only return results from 'books'.\n",
    "Below is the list of the top 24 titles returned from the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_list = [\n",
    "    \"Asteroids: Astronomical and Geological Bodies (Cambridge Planetary Science)\",\n",
    "    \"Asteroids IV (Space Science Series)\",\n",
    "    \"Asteroids: Relics of Ancient Time\",\n",
    "    \"Asteroid Mining 101: Wealth for the New Space Economy\",\n",
    "    \"Asteroid Hunters (TED Books)\",\n",
    "    \"Impact!: Asteroids and the Science of Saving the World (Scientists in the Field Series)\",\n",
    "    \"Asteroid Goddesses: The Mythology, Psychology, and Astrology of the Re-Emerging Feminine\",\n",
    "    \"Asteroid Archetypes: A Primer\",\n",
    "    \"Planets & Asteroids: Relationships in Conjunction\",\n",
    "    \"Kiss My Asteroid: Galaxa Warriors (Paranormal Dating Agency Book 14)\",\n",
    "    \"Asteroids & Meteoroids (Our Galaxy)\",\n",
    "    \"Asteroid Aftermath (Post Apocalyptic Survival Book 2)\",\n",
    "    \"Dawn of Small Worlds: Dwarf Planets, Asteroids, Comets (Astronomers' Universe)\",\n",
    "    \"The Silent War: Book III of The Asteroid Wars (The Grand Tour 12)\",\n",
    "    \"Asteroids: Prospective Energy and Material Resources\",\n",
    "    \"Mechanics of the future: Asteroids\",\n",
    "    \"The Asteroid Ephemeris 1900 to 2050\",\n",
    "    \"A Is for Asteroids, Z Is for Zombies: A Bedtime Book about the Coming Apocalypse\",\n",
    "    \"The Wailing Asteroid\",\n",
    "    \"Comets, Meteors, and Asteroids\",\n",
    "    \"The Wailing Asteroid: A Science Fiction Story\",\n",
    "    \"Catching Stardust: Comets, Asteroids and the Birth of the Solar System (Bloomsbury Sigma)\",\n",
    "    \"Asteroids: The Atari 2600 Game Journal\",\n",
    "    \"Asteroids III (Space Science Series)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize titles\n",
    "\n",
    "book_list_tokenized = []\n",
    "\n",
    "for book in book_list:\n",
    "    text = word_tokenize(book)\n",
    "    book_list_tokenized.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<24x105 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 166 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize the titles using CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit_transform(book_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'asteroids': 15, 'astronomical': 18, 'and': 10, 'geological': 46, 'bodies': 23, 'cambridge': 26, 'planetary': 68, 'science': 79, 'iv': 54, 'space': 86, 'series': 81, 'relics': 76, 'of': 65, 'ancient': 9, 'time': 93, 'asteroid': 14, 'mining': 61, '101': 0, 'wealth': 101, 'for': 41, 'the': 92, 'new': 64, 'economy': 34, 'hunters': 49, 'ted': 91, 'books': 25, 'impact': 51, 'saving': 78, 'world': 102, 'scientists': 80, 'in': 52, 'field': 40, 'goddesses': 47, 'mythology': 63, 'psychology': 73, 'astrology': 16, 're': 74, 'emerging': 35, 'feminine': 38, 'archetypes': 13, 'primer': 71, 'planets': 69, 'relationships': 75, 'conjunction': 30, 'kiss': 56, 'my': 62, 'galaxa': 43, 'warriors': 99, 'paranormal': 67, 'dating': 31, 'agency': 8, 'book': 24, '14': 2, 'meteoroids': 59, 'our': 66, 'galaxy': 44, 'aftermath': 7, 'post': 70, 'apocalyptic': 12, 'survival': 89, 'dawn': 32, 'small': 84, 'worlds': 103, 'dwarf': 33, 'comets': 28, 'astronomers': 17, 'universe': 96, 'silent': 83, 'war': 98, 'iii': 50, 'wars': 100, 'grand': 48, 'tour': 95, '12': 1, 'prospective': 72, 'energy': 36, 'material': 57, 'resources': 77, 'mechanics': 58, 'future': 42, 'ephemeris': 37, '1900': 3, 'to': 94, '2050': 4, 'is': 53, 'zombies': 104, 'bedtime': 20, 'about': 6, 'coming': 29, 'apocalypse': 11, 'wailing': 97, 'meteors': 60, 'fiction': 39, 'story': 88, 'catching': 27, 'stardust': 87, 'birth': 21, 'solar': 85, 'system': 90, 'bloomsbury': 22, 'sigma': 82, 'atari': 19, '2600': 5, 'game': 45, 'journal': 55}\n"
     ]
    }
   ],
   "source": [
    "# Print a list of word requencies\n",
    "\n",
    "print(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 105)\n"
     ]
    }
   ],
   "source": [
    "# Create tfidf matrix\n",
    "tf_matrix = count_vectorizer.transform(book_list).toarray()\n",
    "tfidfTran = TfidfTransformer(norm=\"l2\")\n",
    "tfidfTran.fit(tf_matrix)\n",
    "tfidf_matrix = tfidfTran.transform(tf_matrix)\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix contains the 105 unique words for all of the 24 book titles.\n",
    "\n",
    "Next, we calulate the pairwise cosine similarity of the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.15538444 0.03953647 0.         0.         0.1373826\n",
      "  0.05615971 0.         0.03847733 0.         0.0417603  0.\n",
      "  0.026376   0.         0.11338328 0.04457936 0.         0.02058197\n",
      "  0.         0.16229809 0.09909496 0.07855977 0.03541959 0.16143223]\n",
      " [0.15538444 1.         0.05674252 0.14271707 0.         0.23940094\n",
      "  0.         0.         0.05522244 0.         0.05993414 0.\n",
      "  0.03785469 0.         0.0498646  0.06398004 0.         0.02953912\n",
      "  0.         0.07137674 0.14222051 0.03454964 0.05083399 0.68655767]\n",
      " [0.03953647 0.05674252 1.         0.         0.         0.0923783\n",
      "  0.06506041 0.         0.05031352 0.         0.05460638 0.\n",
      "  0.10365013 0.06134717 0.04543196 0.17518411 0.         0.02691329\n",
      "  0.         0.06503181 0.         0.09460056 0.04631518 0.05895102]\n",
      " [0.         0.14271707 0.         1.         0.05603497 0.08691514\n",
      "  0.09488502 0.06726903 0.         0.03411285 0.         0.04615735\n",
      "  0.         0.11830396 0.         0.0549413  0.0909456  0.1896449\n",
      "  0.16990182 0.         0.09951268 0.05933732 0.04365245 0.14827183]\n",
      " [0.         0.         0.         0.05603497 1.         0.\n",
      "  0.04914709 0.09802847 0.         0.0497113  0.         0.06726326\n",
      "  0.         0.04634208 0.         0.         0.06950762 0.\n",
      "  0.12985203 0.         0.07605523 0.         0.         0.        ]\n",
      " [0.1373826  0.23940094 0.0923783  0.08691514 0.         1.\n",
      "  0.2348098  0.         0.15751522 0.         0.03246793 0.\n",
      "  0.06162843 0.25211775 0.08815359 0.24112321 0.10781239 0.07923651\n",
      "  0.20141185 0.12618403 0.19501298 0.24653092 0.13635842 0.24871877]\n",
      " [0.05615971 0.         0.06506041 0.09488502 0.04914709 0.2348098\n",
      "  1.         0.05900024 0.         0.02991966 0.         0.04048363\n",
      "  0.04340381 0.2181323  0.06453397 0.16973459 0.1176985  0.04449597\n",
      "  0.21988077 0.09237464 0.12878569 0.18841488 0.07657328 0.        ]\n",
      " [0.         0.         0.         0.06726903 0.09802847 0.\n",
      "  0.05900024 1.         0.         0.05967757 0.         0.0807484\n",
      "  0.         0.05563288 0.         0.         0.08344271 0.\n",
      "  0.15588514 0.         0.09130301 0.         0.         0.        ]\n",
      " [0.03847733 0.05522244 0.05031352 0.         0.         0.15751522\n",
      "  0.         0.         1.         0.         0.05314353 0.\n",
      "  0.17673494 0.         0.04421488 0.05673103 0.         0.02619231\n",
      "  0.         0.06328967 0.         0.03063512 0.04507444 0.05737178]\n",
      " [0.         0.         0.         0.03411285 0.0497113  0.\n",
      "  0.02991966 0.05967757 0.         1.         0.         0.12503371\n",
      "  0.         0.08614394 0.         0.         0.0423147  0.05096349\n",
      "  0.07905105 0.         0.04630075 0.         0.         0.        ]\n",
      " [0.0417603  0.05993414 0.05460638 0.         0.         0.03246793\n",
      "  0.         0.         0.05314353 0.         1.         0.\n",
      "  0.03642961 0.         0.04798739 0.06157144 0.         0.02842709\n",
      "  0.         0.06868968 0.         0.03324898 0.04892029 0.06226686]\n",
      " [0.         0.         0.         0.04615735 0.06726326 0.\n",
      "  0.04048363 0.0807484  0.         0.12503371 0.         1.\n",
      "  0.         0.11655945 0.         0.         0.05725509 0.06895757\n",
      "  0.10696222 0.         0.06264851 0.         0.         0.        ]\n",
      " [0.026376   0.03785469 0.10365013 0.         0.         0.06162843\n",
      "  0.04340381 0.         0.17673494 0.         0.03642961 0.\n",
      "  1.         0.0409266  0.03030906 0.11687074 0.         0.01795469\n",
      "  0.         0.19588586 0.         0.13692855 0.03089829 0.03932805]\n",
      " [0.         0.         0.06134717 0.11830396 0.04634208 0.25211775\n",
      "  0.2181323  0.05563288 0.         0.08614394 0.         0.11655945\n",
      "  0.0409266  1.         0.         0.2054848  0.14674812 0.11044403\n",
      "  0.27415039 0.         0.16057178 0.18457291 0.10830444 0.14879652]\n",
      " [0.11338328 0.0498646  0.04543196 0.         0.         0.08815359\n",
      "  0.06453397 0.         0.04421488 0.         0.04798739 0.\n",
      "  0.03030906 0.         1.         0.05122682 0.         0.02365105\n",
      "  0.         0.18649918 0.         0.09027422 0.04070118 0.0518054 ]\n",
      " [0.04457936 0.06398004 0.17518411 0.0549413  0.         0.24112321\n",
      "  0.16973459 0.         0.05673103 0.         0.06157144 0.\n",
      "  0.11687074 0.2054848  0.05122682 1.         0.06815099 0.07031816\n",
      "  0.12731761 0.07332663 0.07457081 0.20017141 0.12101079 0.06647024]\n",
      " [0.         0.         0.         0.0909456  0.06950762 0.10781239\n",
      "  0.1176985  0.08344271 0.         0.0423147  0.         0.05725509\n",
      "  0.         0.14674812 0.         0.06815099 1.         0.03146482\n",
      "  0.2107518  0.         0.12343879 0.07360395 0.05414793 0.        ]\n",
      " [0.02058197 0.02953912 0.02691329 0.1896449  0.         0.07923651\n",
      "  0.04449597 0.         0.02619231 0.05096349 0.02842709 0.06895757\n",
      "  0.01795469 0.11044403 0.02365105 0.07031816 0.03146482 1.\n",
      "  0.05878162 0.03385438 0.0344288  0.05955745 0.05586981 0.03068883]\n",
      " [0.         0.         0.         0.16990182 0.12985203 0.20141185\n",
      "  0.21988077 0.15588514 0.         0.07905105 0.         0.10696222\n",
      "  0.         0.27415039 0.         0.12731761 0.2107518  0.05878162\n",
      "  1.         0.         0.58570693 0.13750468 0.10115752 0.        ]\n",
      " [0.16229809 0.07137674 0.06503181 0.         0.         0.12618403\n",
      "  0.09237464 0.         0.06328967 0.         0.06868968 0.\n",
      "  0.19588586 0.         0.18649918 0.07332663 0.         0.03385438\n",
      "  0.         1.         0.         0.26840594 0.05826013 0.07415483]\n",
      " [0.09909496 0.14222051 0.         0.09951268 0.07605523 0.19501298\n",
      "  0.12878569 0.09130301 0.         0.04630075 0.         0.06264851\n",
      "  0.         0.16057178 0.         0.07457081 0.12343879 0.0344288\n",
      "  0.58570693 0.         1.         0.08053744 0.05924866 0.14775594]\n",
      " [0.07855977 0.03454964 0.09460056 0.05933732 0.         0.24653092\n",
      "  0.18841488 0.         0.03063512 0.         0.03324898 0.\n",
      "  0.13692855 0.18457291 0.09027422 0.20017141 0.07360395 0.05955745\n",
      "  0.13750468 0.26840594 0.08053744 1.         0.10249264 0.03589436]\n",
      " [0.03541959 0.05083399 0.04631518 0.04365245 0.         0.13635842\n",
      "  0.07657328 0.         0.04507444 0.         0.04892029 0.\n",
      "  0.03089829 0.10830444 0.04070118 0.12101079 0.05414793 0.05586981\n",
      "  0.10115752 0.05826013 0.05924866 0.10249264 1.         0.05281252]\n",
      " [0.16143223 0.68655767 0.05895102 0.14827183 0.         0.24871877\n",
      "  0.         0.         0.05737178 0.         0.06226686 0.\n",
      "  0.03932805 0.14879652 0.0518054  0.06647024 0.         0.03068883\n",
      "  0.         0.07415483 0.14775594 0.03589436 0.05281252 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "cos_similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()\n",
    "print(cos_similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix is a bit dense to be able to compare visually. Using the below code, we make this dense matrix more readable.\n",
    "\n",
    "Below, we are converting that very hard to read dense matrix into a sparse one. This helps in two ways. First, it provides a clear representation of what two vectors are being compared. The second way in which it helps is that it removes all 0 values, which are vectors that are not similar at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.9999999999999998\n",
      "  (0, 1)\t0.15538444330493628\n",
      "  (0, 2)\t0.03953647239076911\n",
      "  (0, 5)\t0.13738260333189412\n",
      "  (0, 6)\t0.0561597091669565\n",
      "  (0, 8)\t0.038477330487822196\n",
      "  (0, 10)\t0.041760298462173294\n",
      "  (0, 12)\t0.026376004609617705\n",
      "  (0, 14)\t0.11338328012795557\n",
      "  (0, 15)\t0.044579362061319724\n",
      "  (0, 17)\t0.020581969867405563\n",
      "  (0, 19)\t0.162298086636718\n",
      "  (0, 20)\t0.09909496141958725\n",
      "  (0, 21)\t0.07855977176005863\n",
      "  (0, 22)\t0.03541959014363023\n",
      "  (0, 23)\t0.16143223140481372\n",
      "  (1, 0)\t0.15538444330493628\n",
      "  (1, 1)\t0.9999999999999999\n",
      "  (1, 2)\t0.05674251586071726\n",
      "  (1, 3)\t0.14271707072810455\n",
      "  (1, 5)\t0.2394009372568094\n",
      "  (1, 8)\t0.055222441544710586\n",
      "  (1, 10)\t0.059934138140038014\n",
      "  (1, 12)\t0.037854688832912084\n",
      "  (1, 14)\t0.04986459765657751\n",
      "  :\t:\n",
      "  (22, 16)\t0.0541479279141726\n",
      "  (22, 17)\t0.05586980972168168\n",
      "  (22, 18)\t0.10115751881340787\n",
      "  (22, 19)\t0.05826012822868739\n",
      "  (22, 20)\t0.059248659530618523\n",
      "  (22, 21)\t0.1024926374437245\n",
      "  (22, 22)\t0.9999999999999998\n",
      "  (22, 23)\t0.05281252303581939\n",
      "  (23, 0)\t0.16143223140481372\n",
      "  (23, 1)\t0.6865576669300361\n",
      "  (23, 2)\t0.0589510169492471\n",
      "  (23, 3)\t0.14827182629848656\n",
      "  (23, 5)\t0.24871876926525505\n",
      "  (23, 8)\t0.05737177913421991\n",
      "  (23, 10)\t0.06226686180085136\n",
      "  (23, 12)\t0.039328048274686896\n",
      "  (23, 13)\t0.14879652038194954\n",
      "  (23, 14)\t0.0518054001841562\n",
      "  (23, 15)\t0.06647023797391347\n",
      "  (23, 17)\t0.030688829355084386\n",
      "  (23, 19)\t0.07415482527802607\n",
      "  (23, 20)\t0.14775594272783413\n",
      "  (23, 21)\t0.03589436123044709\n",
      "  (23, 22)\t0.05281252303581939\n",
      "  (23, 23)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# Convert the dense matrix into a sparse one and sort\n",
    "sparse_matrix = sparse.csr_matrix(cos_similarity_matrix)\n",
    "print(sparse_matrix.sorted_indices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.15538444330493628\n",
      "  (0, 1)\t0.9999999999999999\n",
      "  (0, 2)\t0.05674251586071726\n",
      "  (0, 3)\t0.14271707072810455\n",
      "  (0, 5)\t0.2394009372568094\n",
      "  (0, 8)\t0.055222441544710586\n",
      "  (0, 10)\t0.059934138140038014\n",
      "  (0, 12)\t0.037854688832912084\n",
      "  (0, 14)\t0.04986459765657751\n",
      "  (0, 15)\t0.06398004186675199\n",
      "  (0, 17)\t0.029539123776726663\n",
      "  (0, 19)\t0.07137673898161397\n",
      "  (0, 20)\t0.14222051387115958\n",
      "  (0, 21)\t0.03454963912128019\n",
      "  (0, 22)\t0.05083399033785032\n",
      "  (0, 23)\t0.6865576669300361\n"
     ]
    }
   ],
   "source": [
    "# This output shows the cosine similarities between the second book in the list compared to all the books in the list\n",
    "print(sparse_matrix[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most similar are titles 2 and 24:\n",
    "    \n",
    "    - \"Asteroids IV (Space Science Series)\"\n",
    "    - \"Asteroids III (Space Science Series)\"\n",
    "\n",
    "There are many titles that have a cosine similarity score of 0, so we pick the two highest ranked below:\n",
    "\n",
    "    - \"Asteroids: Astronomical and Geological Bodies (Cambridge Planetary Science)\"\n",
    "    - \"Asteroid Mining 101: Wealth for the New Space Economy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No surpise with these results. The two most similar titles are books that appear in the same series and only differ in their number within that series. As for the disimilar results, we had many titles that contained no words in common. In terms of rankings within the entire 24 book list that we compiled, the most similar titles were ranked 2nd and 24th, while the two dissimilar titles were ranked 1st and 4th."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"Question-2\"></a>Question-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the results from question 1 compared to a major search engine\n",
    "\n",
    "    - Compare a book title from the book list with the 1st and 20th results returned from a major search engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The book title from the list in question 1 that I will be using is \"Asteroid Mining 101: Wealth for the New Space Economy\", since it is one of my favorites. Below are the 1st and 20th search results produced from Google for that book title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_list_Google = [\n",
    "    \"Asteroid Mining 101: Wealth for the New Space Economy\", # Book from question 1\n",
    "    \"Asteroid Mining 101: Wealth for the New Space Economy: John S ...\", # Top result from Google\n",
    "    \"The asteroid miner's guide to the galaxy - Daily Herald\" #20th result from Google\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run our same method and output the similarity scores of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.90883417 0.17260796]\n",
      " [0.90883417 1.         0.15687201]\n",
      " [0.17260796 0.15687201 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize titles\n",
    "book_list_Google_tokenized = []\n",
    "\n",
    "for book in book_list_Google:\n",
    "    text = word_tokenize(book)\n",
    "    book_list_Google_tokenized.append(text)\n",
    "    \n",
    "# Vectorize the titles using CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit_transform(book_list_Google)\n",
    "\n",
    "# Create tfidf matrix\n",
    "tf_matrix = count_vectorizer.transform(book_list_Google).toarray()\n",
    "tfidfTran = TfidfTransformer(norm=\"l2\")\n",
    "tfidfTran.fit(tf_matrix)\n",
    "tfidf_matrix = tfidfTran.transform(tf_matrix)\n",
    "\n",
    "# Pairwise\n",
    "cos_similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()\n",
    "print(cos_similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only a 3x3 matrix, we do not need to create a sparse matrix in order to compare. Above we can see that the book from question 1 is more similar to the first Google result than to the 20th Google result. Given that the top result was the name of the book along with a partial of the authors name, it is no surpise that the similarity between it and the input value was very high. While both links returned from Google were indeed about the book in question, the first was a link to Amazon for the book while the second was an article written about the book. Clearly, Google is presenting the most pertinent information here as someone who searches for a book by its title probably wants to purchase it and not mearly read an article reviewing it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
